#!/usr/bin/env python3
from __future__ import annotations

import argparse
import ipaddress
import json
import logging
import re
import subprocess
from datetime import datetime, timezone
from pathlib import Path
from typing import Iterable
from urllib.error import HTTPError, URLError
from urllib.request import Request, urlopen

DOMAIN_PATTERN = re.compile(r"^[a-z0-9.-]+$")


def read_non_comment_lines(path: Path, lowercase: bool = False) -> list[str]:
    values: list[str] = []
    for raw_line in path.read_text(encoding="utf-8").splitlines():
        line = raw_line.split("#", 1)[0].strip()
        if not line:
            continue
        values.append(line.lower() if lowercase else line)
    return values


def parse_asn(value: str) -> int:
    token = value.strip().upper()
    if token.startswith("AS"):
        token = token[2:]
    if not token.isdigit():
        raise ValueError(f"Invalid ASN token: {value}")
    return int(token)


def normalize_domain(value: str) -> str:
    domain = value.strip().lower().strip(".")
    if domain.startswith("*."):
        domain = domain[2:]
    if not domain:
        raise ValueError("Domain cannot be empty")
    if not DOMAIN_PATTERN.match(domain):
        raise ValueError(f"Invalid domain: {value}")
    return domain


def fetch_announced_prefixes(asn: int, timeout: float) -> set[str]:
    url = f"https://stat.ripe.net/data/announced-prefixes/data.json?resource=AS{asn}"
    payload: dict

    try:
        request = Request(url, headers={"User-Agent": "MetaBlock/0.1"})
        with urlopen(request, timeout=timeout) as response:
            payload = json.loads(response.read().decode("utf-8"))
    except (HTTPError, URLError, TimeoutError, OSError):
        # Some environments have urllib DNS/proxy issues while curl still works.
        try:
            result = subprocess.run(
                ["curl", "-fsSL", "--max-time", str(int(timeout)), url],
                check=True,
                capture_output=True,
                text=True,
            )
        except subprocess.CalledProcessError as exc:
            stderr = (exc.stderr or "").strip()
            message = stderr if stderr else f"curl failed with exit code {exc.returncode}"
            raise RuntimeError(message) from exc
        payload = json.loads(result.stdout)

    prefixes: set[str] = set()
    for row in payload.get("data", {}).get("prefixes", []):
        prefix = row.get("prefix")
        if prefix:
            prefixes.add(prefix)
    return prefixes


def sort_networks(
    prefixes: Iterable[str],
) -> list[ipaddress.IPv4Network | ipaddress.IPv6Network]:
    parsed: set[ipaddress.IPv4Network | ipaddress.IPv6Network] = set()
    for prefix in prefixes:
        parsed.add(ipaddress.ip_network(prefix, strict=False))
    return sorted(parsed, key=lambda n: (n.version, int(n.network_address), n.prefixlen))


def write_list(path: Path, values: Iterable[str]) -> None:
    path.parent.mkdir(parents=True, exist_ok=True)
    path.write_text("\n".join(values) + "\n", encoding="utf-8")


def write_json(path: Path, data: object) -> None:
    path.parent.mkdir(parents=True, exist_ok=True)
    path.write_text(json.dumps(data, indent=2, sort_keys=True) + "\n", encoding="utf-8")


def render_dnsmasq(domains: list[str]) -> str:
    lines = [
        "# Generated by scripts/generate_blocklists.py",
        "# Block Meta domains (A + AAAA)",
    ]
    for domain in domains:
        lines.append(f"address=/{domain}/0.0.0.0")
        lines.append(f"address=/{domain}/::")
    return "\n".join(lines) + "\n"


def render_unbound(domains: list[str]) -> str:
    lines = [
        "# Generated by scripts/generate_blocklists.py",
        "# Block Meta domains with NXDOMAIN",
    ]
    for domain in domains:
        lines.append(f'local-zone: "{domain}." always_nxdomain')
    return "\n".join(lines) + "\n"


def build_extension_rules(domains: list[str]) -> list[dict]:
    rules: list[dict] = []
    for index, domain in enumerate(domains, start=1):
        rules.append(
            {
                "id": index,
                "priority": 1,
                "action": {"type": "block"},
                "condition": {"urlFilter": f"||{domain}^"},
            }
        )
    return rules


def render_nginx_robot_map(tokens: list[str]) -> str:
    lines = [
        "# Generated by scripts/generate_blocklists.py",
        "# Use inside http{} in nginx.conf",
        "map $http_user_agent $block_meta_robot {",
        "    default 0;",
    ]
    for token in tokens:
        lines.append(f"    ~*{re.escape(token)} 1;")
    lines.extend(
        [
            "}",
            "",
            "# Example use in a server/location block:",
            "# if ($block_meta_robot) { return 403; }",
        ]
    )
    return "\n".join(lines) + "\n"


def main() -> int:
    root = Path(__file__).resolve().parents[1]

    parser = argparse.ArgumentParser(
        description="Generate Meta blocklists for DNS and browser extensions."
    )
    parser.add_argument(
        "--asn-file",
        type=Path,
        default=root / "blocklists" / "seed_asns.txt",
        help="Path to ASN seed file.",
    )
    parser.add_argument(
        "--domain-file",
        type=Path,
        default=root / "blocklists" / "seed_domains.txt",
        help="Path to domain seed file.",
    )
    parser.add_argument(
        "--robot-file",
        type=Path,
        default=root / "blocklists" / "seed_robot_user_agents.txt",
        help="Path to robot user-agent token file.",
    )
    parser.add_argument(
        "--snapshot-prefix-file",
        type=Path,
        default=root / "blocklists" / "snapshot_prefixes.txt",
        help="Offline prefix snapshot used when live ASN fetch is unavailable.",
    )
    parser.add_argument(
        "--output-dir",
        type=Path,
        default=root / "generated",
        help="Output directory for generated files.",
    )
    parser.add_argument(
        "--timeout",
        type=float,
        default=15.0,
        help="Timeout in seconds for ASN prefix API calls.",
    )
    parser.add_argument(
        "--skip-asn-fetch",
        action="store_true",
        help="Skip live ASN fetch and use cached/generated prefixes only.",
    )
    parser.add_argument(
        "--allow-empty-prefixes",
        action="store_true",
        help="Allow output generation with zero prefixes.",
    )
    parser.add_argument(
        "--strict",
        action="store_true",
        help="Fail immediately if a single ASN fetch fails.",
    )
    parser.add_argument(
        "--log-level",
        default="INFO",
        choices=["DEBUG", "INFO", "WARNING", "ERROR"],
    )
    args = parser.parse_args()

    logging.basicConfig(
        level=getattr(logging, args.log_level),
        format="%(levelname)s %(message)s",
    )

    asn_values = read_non_comment_lines(args.asn_file, lowercase=False)
    asns = sorted({parse_asn(value) for value in asn_values})

    domain_values = read_non_comment_lines(args.domain_file, lowercase=True)
    domains = sorted({normalize_domain(value) for value in domain_values})

    robot_values = read_non_comment_lines(args.robot_file, lowercase=True)
    robot_tokens = sorted({value.strip() for value in robot_values if value.strip()})

    prefixes: set[str] = set()
    failed_asns: dict[int, str] = {}

    if not args.skip_asn_fetch:
        for asn in asns:
            try:
                fetched = fetch_announced_prefixes(asn, timeout=args.timeout)
                if not fetched:
                    raise RuntimeError("No announced prefixes returned")
                prefixes.update(fetched)
                logging.info("Fetched %d prefixes for AS%d", len(fetched), asn)
            except (HTTPError, URLError, TimeoutError, RuntimeError, OSError) as exc:
                failed_asns[asn] = str(exc)
                logging.warning("Failed to fetch AS%d: %s", asn, exc)
                if args.strict:
                    return 1
    else:
        logging.info("Skipping live ASN fetch")

    prefix_cache = args.output_dir / "meta_prefixes.txt"
    if not prefixes and prefix_cache.exists():
        cached = read_non_comment_lines(prefix_cache, lowercase=False)
        if cached:
            prefixes.update(cached)
            logging.warning("Using cached prefix list from %s", prefix_cache)

    if not prefixes and args.snapshot_prefix_file.exists():
        snapshot = read_non_comment_lines(args.snapshot_prefix_file, lowercase=False)
        if snapshot:
            prefixes.update(snapshot)
            logging.warning(
                "Using offline snapshot prefixes from %s", args.snapshot_prefix_file
            )

    if not prefixes and not args.allow_empty_prefixes:
        logging.error(
            "No prefixes available. Run without --skip-asn-fetch or set --allow-empty-prefixes."
        )
        return 1

    networks = sort_networks(prefixes)
    ipv4_prefixes = [str(net) for net in networks if net.version == 4]
    ipv6_prefixes = [str(net) for net in networks if net.version == 6]
    prefix_strings = [str(net) for net in networks]

    args.output_dir.mkdir(parents=True, exist_ok=True)

    write_list(args.output_dir / "meta_asns.txt", [f"AS{asn}" for asn in asns])
    write_list(args.output_dir / "meta_prefixes.txt", prefix_strings)
    write_list(args.output_dir / "meta_ipv4_prefixes.txt", ipv4_prefixes)
    write_list(args.output_dir / "meta_ipv6_prefixes.txt", ipv6_prefixes)
    write_list(args.output_dir / "meta_domains.txt", domains)
    write_list(args.output_dir / "meta_robot_user_agents.txt", robot_tokens)

    (args.output_dir / "dnsmasq-meta-block.conf").write_text(
        render_dnsmasq(domains), encoding="utf-8"
    )
    (args.output_dir / "unbound-meta-block.conf").write_text(
        render_unbound(domains), encoding="utf-8"
    )
    (args.output_dir / "nginx-meta-robots.conf").write_text(
        render_nginx_robot_map(robot_tokens), encoding="utf-8"
    )

    rules = build_extension_rules(domains)
    chromium_rules = root / "extensions" / "chromium" / "rules" / "rules_1.json"
    firefox_rules = root / "extensions" / "firefox" / "rules" / "rules_1.json"
    write_json(chromium_rules, rules)
    write_json(firefox_rules, rules)

    metadata = {
        "generated_at_utc": datetime.now(tz=timezone.utc).isoformat(),
        "source": {
            "asn_api": "https://stat.ripe.net/data/announced-prefixes/data.json?resource=AS<ASN>",
            "asn_file": str(args.asn_file),
            "domain_file": str(args.domain_file),
            "robot_file": str(args.robot_file),
            "snapshot_prefix_file": str(args.snapshot_prefix_file),
        },
        "counts": {
            "asns": len(asns),
            "prefixes": len(prefix_strings),
            "ipv4_prefixes": len(ipv4_prefixes),
            "ipv6_prefixes": len(ipv6_prefixes),
            "domains": len(domains),
            "robot_user_agent_tokens": len(robot_tokens),
        },
        "failed_asn_fetches": failed_asns,
    }
    write_json(args.output_dir / "metadata.json", metadata)

    logging.info("Generated outputs in %s", args.output_dir)
    return 0


if __name__ == "__main__":
    raise SystemExit(main())
